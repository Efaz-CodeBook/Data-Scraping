{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef22d9-1962-4bc8-baae-649c11ed350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding out the right Method\n",
    "\"\"\"\n",
    "Scrapping Data from Ryans Computers:\n",
    "\n",
    "Step 1: Checking the website, whether it is-\n",
    "    1. API\n",
    "    2. HTML\n",
    "Step 2: If it is API based: Use API Base Scraping\n",
    "        If it is HTML based:\n",
    "            Find if it works without JavaScript: >inspect >elements > Right(Ctrl+Shift+P)>Run-JavaScript=Disable>Reload Wesite:\n",
    "                If the page shoes the data without interruption\n",
    "                than use: Scrapy\n",
    "                Or use: Selenium\n",
    "                \n",
    "Notes:\n",
    "    Scrapy is faster than Selenium\n",
    "    It works when the data is stored in HTML\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a315b3-b91d-476e-80fa-5a6ec736c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the platform\n",
    "\"\"\"\n",
    "Step 1: Open cmd and Install Scrapy>\n",
    "            pip install scrapy\n",
    "Step 2: Open any terminal (cmd, bash, hyper or any) into the selected folder\n",
    "Step 3: Creating Scrapy environment>\n",
    "            scrapy startproject ryans .\n",
    "Step 4: Initiating a spider>\n",
    "            scrapy genspider laptop ryanscomputers.com\n",
    "Step 5: Open vs code in working folder and set user agent\n",
    "        Search the user agent and copy it to the settings.py\n",
    "            USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.82\"\n",
    "            ROBOTSTXT_OBEY = False\n",
    "Step 6: Start coding in spiders/laptop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bbbea-947b-43cc-bdbe-f812af93579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding in laptop.py\n",
    "\n",
    "#Default Code:\n",
    "\n",
    "\"\"\"\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class LaptopSpider(scrapy.Spider):\n",
    "    name = \"laptop\"\n",
    "    allowed_domains = [\"ryanscomputers.com\"]\n",
    "    start_urls = [\"https://ryanscomputers.com\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "# Edited code:\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class LaptopSpider(scrapy.Spider):\n",
    "    name = \"laptop\"\n",
    "\n",
    "    #Passing the url for the page of data\n",
    "    start_urls = [\"https://www.ryanscomputers.com/category/laptop-all-laptop?limit=100&osp=0\"]\n",
    "\n",
    "    #Function for the data extraction\n",
    "    def parse(self, response):\n",
    "        laptops = response.xpath('//div[@class=\"cus-col-2 cus-col-3 cus-col-4 cus-col-5 category-single-product mb-2 context1\"]')\n",
    "        \n",
    "        #For loop for iterate all items\n",
    "        for i, laptop in enumerate(laptops):\n",
    "                    \n",
    "            yield{\n",
    "                'title': response.xpath(f'(//img[@class=\"card-img-top\"]/@alt)[{i}]').get(),\n",
    "                'href' : response.xpath(f'(//p[@class=\"card-text p-0 m-0 grid-view-text\"]/a/@href)[{i}]').get(),\n",
    "                'processor_Type' : response.xpath(f'(//li[contains(text(), \"Processor\")]/text())[{i}]').get(),\n",
    "                'RAM' : response.xpath(f'(//li[contains(text(), \"RAM\")]/text())[{i}]').get(),\n",
    "                'Proc_Generation' : response.xpath(f'(//li[contains(text(), \"Generation\")]/text())[{i}]').get(),\n",
    "                'Display' : response.xpath(f'(//li[contains(text(), \"Display\")]/text())[{i}]').get()\n",
    "            }\n",
    "        \n",
    "        #Calling Next Page\n",
    "        next_page = response.xpath('//a[@aria-label=\"Next Â»\"]/@href').get()\n",
    "        if next_page:\n",
    "            yield scrapy.Request(url=next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e7f7f-ad20-45b4-8d66-cf8891b0f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the data:\n",
    "\"\"\"\n",
    "Need to run the following command in terminal>\n",
    "        scrapy crawl laptop -o ryansLaptop.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
